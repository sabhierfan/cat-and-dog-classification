{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sabhierfan/cat-and-dog-classification/blob/main/Cat_and_dog_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1STDTCZZW5g6"
      },
      "source": [
        "# PROJECT: Pet Classification Tensorflow Model Using CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcjrxt5iKSt4"
      },
      "source": [
        "# Project Objective\n",
        "To build a CNN model that classifies the given pet images correctly into dog and cat images.\n",
        "\n",
        "The project scope document specifies the requirements for the project “Pet Classification Model Using CNN.” Apart from specifying the functional and nonfunctional requirements for the project, it also serves as an input for project scoping."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDsKEqt4KZQ9"
      },
      "source": [
        "# Project Description and Scope\n",
        "We are provided with the following resources that can be used as inputs for your model:\n",
        "\n",
        "A collection of images of pets, that is​, ​cats and dogs. These images are of different sizes with varied lighting conditions.\n",
        "Code template containing the following code blocks: a. Import modules (part 1) b. Set hyper parameters (part 2) c. Read image data set (part 3) d. Run TensorFlow model (part 4) You are expected to write the code for CNN image classification model (between Parts 3 and 4) using TensorFlow that trains on the data and calculates the accuracy score on the test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NguHQ6krKseW"
      },
      "source": [
        "# Project Guidelines\n",
        "Begin by extracting ipynb file and the data in the same folder. The CNN model (cnn_model_fn) should have the following layers: ● Input layer ● Convolutional layer 1 with 32 filters of kernel size[5,5] ● Pooling layer 1 with pool size**[2,2] **and stride 2 ● Convolutional layer 2 with 64 filters of kernel size[5,5] ● Pooling layer 2 with pool size[2,2] and stride 2 ● Dense layer whose output size is fixed in the hyper parameter: fc_size=32 ● Dropout layer with dropout probability 0.4 Predict the class by doing a softmax on the output of the dropout lay bold text\n",
        "\n",
        "This should be followed by training and evaluation: For the training step, define the loss function and minimize it ● For the evaluation step, calculate the accuracy Run the program for 100, 200, and 300 iterations, respectively. Follow this by a report on the final accuracy and loss on the evaluation data. Prerequisites To execute this project, refer to the installation guide in the downloads section of LMS."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MoZku5AqV_39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6NQX-5ZXj_X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83248c71-915c-4abb-f6ce-686f52315d80"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "631031f4"
      },
      "source": [
        "# Pet Classification Tensorflow Model Using CNN\n",
        "\n",
        "## Project Objective\n",
        "To build a CNN model that classifies the given pet images correctly into dog and cat images.\n",
        "\n",
        "The project scope document specifies the requirements for the project “Pet Classification Model Using CNN.” Apart from specifying the functional and nonfunctional requirements for the project, it also serves as an input for project scoping.\n",
        "\n",
        "## Project Description and Scope\n",
        "We are provided with a collection of images of pets (cats and dogs) of different sizes with varied lighting conditions. The goal is to build a CNN image classification model using TensorFlow that trains on this data and calculates the accuracy score on the test data.\n",
        "\n",
        "## Project Guidelines\n",
        "The CNN model (`cnn_model_fn`) should have the following layers:\n",
        "- Input layer\n",
        "- Convolutional layer 1 with 32 filters of kernel size [5,5]\n",
        "- Pooling layer 1 with pool size [2,2] and stride 2\n",
        "- Convolutional layer 2 with 64 filters of kernel size [5,5]\n",
        "- Pooling layer 2 with pool size [2,2] and stride 2\n",
        "- Dense layer whose output size is fixed in the hyper parameter: `fc_size=32`\n",
        "- Dropout layer with dropout probability 0.4\n",
        "\n",
        "The class prediction is done by applying a softmax activation on the output of the dropout layer.\n",
        "\n",
        "The training step involves defining the loss function and minimizing it. For the evaluation step, the accuracy is calculated.\n",
        "\n",
        "The program should be run for 100, 200, and 300 iterations, respectively, followed by a report on the final accuracy and loss on the evaluation data.\n",
        "\n",
        "## Model Architecture\n",
        "\n",
        "The CNN model implemented in this notebook follows the specifications outlined in the project guidelines. It consists of the following layers:\n",
        "\n",
        "1.  **Input Layer:** The input layer receives the image data. Since the images are grayscale and resized to 64x64 pixels, the input shape is (64, 64, 1).\n",
        "2.  **Convolutional Layer 1:** This layer applies 32 filters of size [5, 5] with a ReLU activation function. Convolutional layers learn features from the input images by convolving the filters over the image data.\n",
        "3.  **Pooling Layer 1:** This layer performs max pooling with a pool size of [2, 2] and a stride of 2. Pooling layers reduce the spatial dimensions of the feature maps, helping to reduce computation and control overfitting.\n",
        "4.  **Convolutional Layer 2:** This layer applies 64 filters of size [5, 5] with a ReLU activation function.\n",
        "5.  **Pooling Layer 2:** This layer performs max pooling with a pool size of [2, 2] and a stride of 2.\n",
        "6.  **Flatten Layer:** This layer flattens the output of the pooling layers into a 1D vector, which can be fed into the dense layers.\n",
        "7.  **Dense Layer:** This is a fully connected layer with 32 units and a ReLU activation function. Dense layers learn global patterns in the data. The output size is determined by the `fc_size` hyperparameter.\n",
        "8.  **Dropout Layer:** This layer applies dropout with a probability of 0.4. Dropout is a regularization technique that randomly sets a fraction of the input units to zero during training, which helps prevent overfitting.\n",
        "9.  **Output Layer:** This is a dense layer with 2 units (for the two classes: cat and dog) and a softmax activation function. The softmax function outputs a probability distribution over the classes, indicating the model's confidence in each class prediction.\n",
        "\n",
        "The model is compiled using the Adam optimizer and the categorical crossentropy loss function, which is suitable for multi-class classification problems. The accuracy metric is used to evaluate the model's performance during training and evaluation.\n",
        "\n",
        "## Data Preparation\n",
        "\n",
        "The image data is loaded from the specified directories and preprocessed for training and evaluation. The preprocessing steps include:\n",
        "\n",
        "*   Resizing the images to a fixed size (64x64 pixels).\n",
        "*   Converting the images to grayscale.\n",
        "*   Labeling the images using one-hot encoding (e.g., [1, 0] for cats and [0, 1] for dogs).\n",
        "*   Saving the preprocessed images and labels as NumPy arrays.\n",
        "\n",
        "Data augmentation is applied to the training data using `ImageDataGenerator` to increase the size and diversity of the training set. This includes random rotations, shifts, shear transformations, zoom, and horizontal flips.\n",
        "\n",
        "The `flow_from_directory` method of `ImageDataGenerator` is used to create data generators that load images in batches and apply the specified augmentations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wONr1tOluHw"
      },
      "source": [
        "import cv2                 # working with, mainly resizing, images\n",
        "import numpy as np         # dealing with arrays\n",
        "import os                  # dealing with directories\n",
        "from random import shuffle # mixing up or currently ordered data that might lead our network astray in training.\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__ZZSCZqlz6B"
      },
      "source": [
        "TRAIN_DIR = '/content/drive/My Drive/Dataset_Pet_Classification/data/train'\n",
        "TEST_DIR = '/content/drive/My Drive/Dataset_Pet_Classification/data/test'\n",
        "IMG_SIZE = 224\n",
        "LR = 1e-3\n",
        "\n",
        "MODEL_NAME = 'pet_classifier_2_Conv_basic'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDPUw_7WnbaS"
      },
      "source": [
        "def label_img(folder_name):\n",
        "\n",
        "    # conversion to one-hot array [cat,dog]\n",
        "    #                            [much cat, no dog]\n",
        "    if folder_name == 'cats': return [1,0]\n",
        "    #                             [no cat, very doggo]\n",
        "    elif folder_name == 'dogs': return [0,1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQovP4u-nvJp"
      },
      "source": [
        "def create_train_data():\n",
        "    training_data = []\n",
        "    for n, folder in enumerate(os.listdir(TRAIN_DIR)):\n",
        "        images = os.listdir(os.path.join(TRAIN_DIR, folder))\n",
        "        for i, image in enumerate(images):\n",
        "            label = label_img(folder)\n",
        "            path = os.path.join(TRAIN_DIR, folder, image)\n",
        "            img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "            training_data.append((np.array(img), np.array(label)))\n",
        "    shuffle(training_data)\n",
        "    images = np.array([item[0] for item in training_data])\n",
        "    labels = np.array([item[1] for item in training_data])\n",
        "    np.save('train_images.npy', images)\n",
        "    np.save('train_labels.npy', labels)\n",
        "    return training_data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XI-0VoThqa5d"
      },
      "source": [
        "X = create_train_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N1UFwctqnqU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f242ec04-167f-48cd-9cad-7510858b1fe5"
      },
      "source": [
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn-mZZ_CqsX0"
      },
      "source": [
        "def create_test_data():\n",
        "    test_data = []\n",
        "    for n, folder in enumerate(os.listdir(TEST_DIR)):\n",
        "        images = os.listdir(os.path.join(TEST_DIR, folder))\n",
        "        for i, image in enumerate(images):\n",
        "            label = label_img(folder)\n",
        "            path = os.path.join(TEST_DIR, folder, image)\n",
        "            img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "            img = img / 255\n",
        "            test_data.append((np.array(img), np.array(label)))\n",
        "    shuffle(test_data)\n",
        "    images = np.array([item[0] for item in test_data])\n",
        "    labels = np.array([item[1] for item in test_data])\n",
        "    np.save('test_images.npy', images)\n",
        "    np.save('test_labels.npy', labels)\n",
        "    return test_data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5sKdmbWr7HX"
      },
      "source": [
        "Y = create_test_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tflearn\n"
      ],
      "metadata": {
        "id": "8rQOvBvgqh1B",
        "outputId": "6f89f911-84c1-4a8a-80c7-a061205e4e8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tflearn in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tflearn) (1.25.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tflearn) (1.16.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from tflearn) (10.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_E_lpLBQimk"
      },
      "source": [
        "# Model Specification\n",
        "The CNN model (cnn_model_fn) should have the following layers: ● Input layer ● Convolutional layer 1 with 32 filters of kernel size[5,5] ● Pooling layer 1 with pool size**[2,2] **and stride 2 ● Convolutional layer 2 with 64 filters of kernel size[5,5] ● Pooling layer 2 with pool size[2,2] and stride 2 ● Dense layer whose output size is fixed in the hyper parameter: fc_size=32 ● Dropout layer with dropout probability 0.4 Predict the class by doing a softmax on the output of the dropout lay bold text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSPcLWXbsCXM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7422fa5-f663-461b-b9a8-5d1d7a0276de"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "def model(input_shape, num_classes, fc_size=32, dropout_prob=0.4):\n",
        "    model = Sequential([\n",
        "        # Input layer\n",
        "        Conv2D(32, (5, 5), activation='relu', input_shape=input_shape),\n",
        "        # Pooling layer 1\n",
        "        MaxPooling2D(pool_size=(2, 2), strides=2),\n",
        "        # Convolutional layer 2\n",
        "        Conv2D(64, (5, 5), activation='relu'),\n",
        "        # Pooling layer 2\n",
        "        MaxPooling2D(pool_size=(2, 2), strides=2),\n",
        "        # Flatten layer\n",
        "        Flatten(),\n",
        "        # Dense layer\n",
        "        Dense(fc_size, activation='relu'),\n",
        "        # Dropout layer\n",
        "        Dropout(dropout_prob),\n",
        "        # Output layer\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Define input shape and number of classes\n",
        "input_shape = (IMG_SIZE, IMG_SIZE, 1)  # Assuming grayscale images\n",
        "num_classes = 2  # Assuming binary classification\n",
        "\n",
        "# Create the model\n",
        "model = model(input_shape, num_classes)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 220, 220, 32)      832       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 110, 110, 32)      0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 106, 106, 64)      51264     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 53, 53, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 179776)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 32)                5752864   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 32)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 66        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5805026 (22.14 MB)\n",
            "Trainable params: 5805026 (22.14 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "IMG_SIZE = 64  # Adjust this value to the actual size of your images\n",
        "BATCH_SIZE = 32  # Adjust based on your memory capacity\n",
        "\n",
        "# Define paths to your training and validation directories\n",
        "train_dir = TRAIN_DIR\n",
        "\n",
        "\n",
        "# Create ImageDataGenerator for training and validation\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   rotation_range=20,\n",
        "                                   width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True)\n",
        "\n",
        "\n",
        "\n",
        "# Create data generators\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    color_mode='grayscale',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gX50DPvyuKh1",
        "outputId": "68e969c1-7ba7-4af2-dada-f4aa1fcc8b07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 40 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 2  # Adjust based on your requirement\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
        "    epochs=EPOCHS\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "w_6xajfuvqZQ",
        "outputId": "0333b3ec-d6fb-46cf-94c8-e10f9d6c4ab8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "Graph execution error:\n\nDetected at node sequential/dense/Relu defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-21-74735e62e8d3>\", line 3, in <cell line: 3>\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 590, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/sequential.py\", line 398, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\", line 515, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\", line 672, in _run_internal_graph\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py\", line 255, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/activations.py\", line 306, in relu\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5395, in relu\n\nMatrix size-incompatible: In[0]: [32,10816], In[1]: [179776,32]\n\t [[{{node sequential/dense/Relu}}]] [Op:__inference_train_function_1302]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-74735e62e8d3>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m  \u001b[0;31m# Adjust based on your requirement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node sequential/dense/Relu defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-21-74735e62e8d3>\", line 3, in <cell line: 3>\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 590, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/sequential.py\", line 398, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\", line 515, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\", line 672, in _run_internal_graph\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py\", line 255, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/activations.py\", line 306, in relu\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5395, in relu\n\nMatrix size-incompatible: In[0]: [32,10816], In[1]: [179776,32]\n\t [[{{node sequential/dense/Relu}}]] [Op:__inference_train_function_1302]"
          ]
        }
      ]
    }
  ]
}